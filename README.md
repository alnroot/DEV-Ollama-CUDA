# ğŸ¤– Ollama Chat - Interfaz Web Optimizada

Interfaz web moderna y optimizada para interactuar con mÃºltiples servicios de Ollama con monitoreo en tiempo real del sistema.

## âœ¨ CaracterÃ­sticas Principales

- ğŸ¯ **Chat en tiempo real** con modelos Ollama locales
- ğŸ“¥ **Descarga de modelos** populares con un click
- ğŸ”„ **SelecciÃ³n de servicios** (General, Code, Text)
- ğŸ“Š **EstadÃ­sticas tÃ©cnicas** avanzadas (GPU, CPU, Memoria)
- ğŸ³ **GestiÃ³n de Docker** integrada
- ğŸ”Œ **Test de conectividad** de servicios
- ğŸ“± **DiseÃ±o responsivo** y moderno
- ğŸ—ï¸ **Arquitectura modular** design patterns

## ğŸ›ï¸ Arquitectura

La aplicaciÃ³n sigue una arquitectura modular con separaciÃ³n clara de responsabilidades:

```
ğŸ“ OllamaTest/
â”œâ”€â”€ ğŸ“„ main.py              # ğŸš€ Punto de entrada principal
â”œâ”€â”€ ğŸ“„ requirements.txt     # ğŸ“¦ Dependencias del proyecto  
â”œâ”€â”€ ğŸ“„ docker-compose.yml   # ğŸ³ ConfiguraciÃ³n de contenedores
â”œâ”€â”€ ğŸ“„ README.md           # ğŸ“– DocumentaciÃ³n
â”œâ”€â”€ ğŸ“ .venv/              # ğŸ Entorno virtual Python
â””â”€â”€ ğŸ“ src/                # ğŸ“‚ CÃ³digo fuente modular
    â”œâ”€â”€ ğŸ“ config/         # âš™ï¸ ConfiguraciÃ³n (Singleton)
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â””â”€â”€ settings.py    # Settings centralizados
    â”œâ”€â”€ ğŸ“ core/           # ğŸ§  LÃ³gica de negocio principal
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ interfaces.py  # Interfaces (InversiÃ³n de dependencias)
    â”‚   â””â”€â”€ chat_controller.py # Controlador principal
    â”œâ”€â”€ ğŸ“ services/       # ğŸ”§ Servicios especializados
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ ollama_service.py     # ComunicaciÃ³n con Ollama
    â”‚   â”œâ”€â”€ model_repository.py  # GestiÃ³n de modelos
    â”‚   â”œâ”€â”€ chat_history.py      # Historial (Singleton)
    â”‚   â”œâ”€â”€ technical_stats.py   # EstadÃ­sticas tÃ©cnicas
    â”‚   â””â”€â”€ docker_commands.py   # Comandos Docker
    â””â”€â”€ ğŸ“ web/            # ğŸŒ Interfaz web
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ app.py         # AplicaciÃ³n Flask con factory pattern
        â””â”€â”€ templates/
            â””â”€â”€ index.html # Interfaz Ãºnica con panel futurista
```

## ğŸ”§ Patrones de DiseÃ±o Implementados

### 1. **Singleton Pattern**
- `Settings`: ConfiguraciÃ³n Ãºnica para toda la aplicaciÃ³n
- `ChatHistory`: Historial compartido entre sesiones

### 2. **Dependency Inversion Pattern**
- Interfaces abstractas definidas en `core/interfaces.py`
- Implementaciones concretas en `services/`
- InyecciÃ³n de dependencias en el controlador principal

### 3. **Factory Pattern**
- `create_app()` para inicializar la aplicaciÃ³n Flask con todas sus dependencias

## ğŸš€ InstalaciÃ³n y Uso

### Prerrequisitos
- Python 3.8+
- Docker y Docker Compose
- Servicios Ollama corriendo en puertos 11434, 11435, 11436

### InstalaciÃ³n

1. **Clonar el repositorio:**
```bash
git clone <repository-url>
cd OllamaTest
```

2. **Instalar dependencias:**
```bash
pip install -r requirements.txt
```

3. **Iniciar la aplicaciÃ³n:**
```bash
python main.py
```

4. **Abrir en el navegador:**
```
http://localhost:5000
```

## ğŸ“‹ Funcionalidades

### Chat
- Seleccionar servicio (General/Code/Text)
- Elegir modelo disponible
- ConversaciÃ³n en tiempo real
- Historial de chat persistente

### GestiÃ³n de Modelos
- Ver modelos disponibles por servicio
- Descargar modelos populares predefinidos
- Modelos sugeridos: `llama3.2:3b`, `llama3.2:1b`, `qwen2.5:7b`, `codellama:7b`, `mistral:7b`

### EstadÃ­sticas TÃ©cnicas
- InformaciÃ³n de GPU (CUDA, memoria, compute capability)
- MÃ©tricas del modelo activo (arquitectura, parÃ¡metros, quantizaciÃ³n)
- Uso de CPU, RAM y disco
- Rendimiento en tiempo real (tokens/segundo, tiempos de inferencia)

### GestiÃ³n de Servicios
- Iniciar/detener/reiniciar contenedores Docker
- Estado en tiempo real de servicios
- Test de conectividad automÃ¡tico

## ğŸ”„ API Endpoints

### Chat y Modelos
- `GET /` - Interfaz web principal
- `GET /api/models/<service_type>` - Obtener modelos disponibles
- `GET /api/popular-models` - Obtener modelos populares
- `POST /api/chat` - Enviar mensaje al modelo
- `POST /api/download` - Descargar modelo
- `GET /api/status` - Estado de servicios

### Historial
- `GET /api/history` - Obtener historial
- `POST /api/clear-history` - Limpiar historial

### Funcionalidades TÃ©cnicas
- `GET /api/technical-stats` - EstadÃ­sticas tÃ©cnicas avanzadas
- `POST /api/docker/start` - Iniciar servicios Docker
- `POST /api/docker/stop` - Detener servicios Docker
- `POST /api/docker/restart` - Reiniciar servicios Docker
- `GET /api/docker/status` - Estado de contenedores
- `GET /api/test-connections` - Test de conectividad

## ğŸ› ï¸ ConfiguraciÃ³n

### Settings (Singleton)
```python
# src/config/settings.py
services = {
    "general": "http://localhost:11434",
    "code": "http://localhost:11435", 
    "text": "http://localhost:11436"
}

popular_models = [
    "llama3.2:3b", "llama3.2:1b", 
    "qwen2.5:7b", "codellama:7b", "mistral:7b"
]
```

## ğŸ”— Dependencias

```txt
requests>=2.31.0      # HTTP requests
docker>=6.1.0         # Docker integration  
python-dotenv>=1.0.0  # Environment variables
flask>=2.3.0          # Web framework
flask-cors>=4.0.0     # CORS support
psutil>=5.9.0         # System monitoring
```

## ğŸ“± Interfaz de Usuario

- **DiseÃ±o futurista** con tema cyberpunk y efectos de neÃ³n
- **Panel de estadÃ­sticas en tiempo real** con estilo consola hacker
- **Barras de progreso animadas** que cambian de color segÃºn la carga
- **Indicadores LED pulsantes** para estado de servicios
- **Responsive** para desktop y mÃ³vil
- **Chat estilo messenger** con mensajes diferenciados
- **Efectos de escaneo** y animaciones fluidas

## ğŸ§ª Extensibilidad

La arquitectura modular permite fÃ¡cil extensiÃ³n:

1. **Nuevos servicios**: Implementar interfaces en `services/`
2. **Nuevas funcionalidades**: Agregar al controlador principal
3. **Nuevas interfaces**: Definir en `core/interfaces.py`
4. **ConfiguraciÃ³n**: Modificar `config/settings.py`

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT.

---

**Desarrollado con â¤ï¸ usando arquitectura modular y patrones de diseÃ±o modernos** 

## âš¡ Optimizaciones de Rendimiento

### ğŸš€ Mejoras Implementadas

1. **ConfiguraciÃ³n GPU Optimizada**
   - GPU principal usa toda la memoria disponible (`OLLAMA_GPU_LAYERS=-1`)
   - Servicios secundarios solo usan CPU para evitar competencia
   - Flash Attention habilitado para mayor eficiencia
   - LÃ­mites de modelos cargados simultÃ¡neamente

2. **Connection Pooling & Caching**
   - ReutilizaciÃ³n de conexiones HTTP con `requests.Session`
   - Cache de modelos disponibles (60 segundos)
   - Cache de estadÃ­sticas tÃ©cnicas (30 segundos)
   - Retry automÃ¡tico con backoff exponencial

3. **Monitoreo Inteligente**
   - EstadÃ­sticas actualizadas cada 15 segundos (antes 5s)
   - Cache de logs de Docker para reducir subprocess calls
   - DetecciÃ³n de GPU con cache de 5 minutos

4. **ConfiguraciÃ³n de Modelos Optimizada**
   - Contexto reducido para respuestas mÃ¡s rÃ¡pidas (2048 tokens)
   - LÃ­mite de predicciÃ³n ajustado (512 tokens)
   - ParÃ¡metros optimizados para RTX 4070 Laptop

## ğŸ”§ Comandos de Despliegue

### Modo Optimizado (Solo servicio principal)
```bash
docker-compose up -d ollama
```

### Modo Multi-Servicio (Si necesitas separaciÃ³n)
```bash
docker-compose --profile multi-service up -d
```

## ğŸ“Š Monitoreo de Rendimiento

El panel de estadÃ­sticas muestra:
- **GPU**: UtilizaciÃ³n, memoria y drivers
- **CPU**: Uso, cores y frecuencia
- **Memoria**: RAM disponible y utilizada
- **Performance**: Tokens/seg, tiempo de inferencia
- **Servicios**: Estado de conexiÃ³n

## ğŸ¯ Recomendaciones Adicionales

### este caso de uso es mi rtx 4070:

1. **Usar modelos cuantizados menores**:
   ```bash
   # Mejores opciones para tu GPU
   ollama pull llama3.2:1b    # MÃ¡s rÃ¡pido
   ollama pull llama3.2:3b    # Buen balance
   ollama pull qwen2.5:3b     # Eficiente para cÃ³digo
   ```

2. **ConfiguraciÃ³n de energÃ­a**:
   - AsegÃºrate que la laptop estÃ© conectada
   - Modo de rendimiento alto en Windows
   - GPU no limitada por thermal throttling

3. **Memoria del sistema**:
   - MÃ­nimo 16GB RAM recomendado
   - Cerrar aplicaciones innecesarias

### Benchmarks Esperados

Con las optimizaciones aplicadas:
- **Tiempo de carga inicial**: ~15-20 segundos (antes 25s)
- **Tokens por segundo**: 15-25 (modelos 3B), 8-15 (modelos 7B)
- **Tiempo de respuesta promedio**: 2-5 segundos

## ğŸ³ Docker Compose Optimizado

ConfiguraciÃ³n actual optimizada para una sola GPU:
- Servicio principal: Toda la GPU + 8GB RAM
- Servicios secundarios: Solo CPU (opcional)
- Memory mapping y lock optimizados

## ğŸ’¡ Troubleshooting

### Si sigues experimentando lentitud:

1. **Verificar uso de GPU**:
   ```bash
   nvidia-smi -l 1
   ```

2. **Monitorear logs en tiempo real**:
   ```bash
   docker logs -f ollama-service
   ```

3. **Verificar temperaturas**:
   - GPU < 83Â°C
   - CPU < 85Â°C

4. **Liberar memoria de modelos**:
   ```bash
   curl -X POST http://localhost:11434/api/generate -d '{"model": "", "keep_alive": 0}'
   ```

## ğŸš€ InstalaciÃ³n RÃ¡pida

```bash
# Clonar repositorio
git clone <repo-url>
cd OllamaTest

# Crear entorno virtual
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# o
.venv\Scripts\activate     # Windows

# Instalar dependencias
pip install -r requirements.txt

# Iniciar servicios optimizados
docker-compose up -d ollama

# Iniciar interfaz web
python main.py
```

Visita: http://localhost:5000 
